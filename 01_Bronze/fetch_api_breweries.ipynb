{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d02b0fa1-9c64-4972-85bd-b1dab5c58942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Layer\n",
    "Bronze Layer: Persist the raw data from the API in its native format or any format you find suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11493f1-6587-48d8-9aee-b187f93bdf95",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libs"
    }
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col, count, when, current_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70ce846-1cb9-499a-ae1f-bad8ddb97835",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function to Fetch API"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_breweries_from_api(\n",
    "    params: dict = None,\n",
    "    per_page: int = 200,\n",
    "    sort: str = None,\n",
    "    version: str = \"v1\",\n",
    "    retries: int = 3\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Fetches paginated brewery data from the Open Brewery DB API with retry logic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : dict, optional\n",
    "        Filters for the API request (e.g., by_city, by_state, etc). Default is None.\n",
    "    per_page : int, optional\n",
    "        Number of items per page (maximum 200). Default is 200.\n",
    "    sort : str, optional\n",
    "        Fields for sorting (e.g., 'name,desc'). Default is None.\n",
    "    version : str, optional\n",
    "        API version to use. Default is \"v1\".\n",
    "    retries : int, optional\n",
    "        Number of times to retry a failed request per page. Default is 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of dictionaries containing brewery data from the API.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.openbrewerydb.org/{version}/breweries\"\n",
    "    all_data = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        query = params.copy() if params else {}\n",
    "        query.update({\"page\": page, \"per_page\": per_page})\n",
    "        if sort:\n",
    "            query[\"sort\"] = sort\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                response = requests.get(url, params=query)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                break\n",
    "            except requests.RequestException:\n",
    "                attempt += 1\n",
    "                if attempt == retries:\n",
    "                    return all_data\n",
    "        if not data:\n",
    "            break\n",
    "        all_data.extend(data)\n",
    "        page += 1\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a7dd2d-24f4-4a49-a738-73c69f5be644",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Quality - Schema Validation"
    }
   },
   "outputs": [],
   "source": [
    "def get_brewery_schema():\n",
    "    \"\"\"\n",
    "    Returns the schema for the breweries DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    StructType\n",
    "        A StructType object defining the schema for brewery data, including required and optional fields.\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"id\", StringType(), False),  \n",
    "        StructField(\"name\", StringType(), False),  \n",
    "        StructField(\"brewery_type\", StringType(), False),  \n",
    "        StructField(\"address_1\", StringType(), True),\n",
    "        StructField(\"address_2\", StringType(), True),\n",
    "        StructField(\"address_3\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), False),  \n",
    "        StructField(\"state_province\", StringType(), False),  \n",
    "        StructField(\"postal_code\", StringType(), False),  \n",
    "        StructField(\"country\", StringType(), False),  \n",
    "        StructField(\"longitude\", StringType(), True),\n",
    "        StructField(\"latitude\", StringType(), True),\n",
    "        StructField(\"phone\", StringType(), True),\n",
    "        StructField(\"website_url\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),  \n",
    "        StructField(\"street\", StringType(), True) \n",
    "    ])\n",
    "\n",
    "def create_brewery_df(data):\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame for brewery data using the predefined schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of dict\n",
    "        List of dictionaries containing brewery data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        Spark DataFrame with the brewery schema applied.\n",
    "    \"\"\"\n",
    "    schema = get_brewery_schema()\n",
    "    return spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90813932-9308-43a7-a735-db44f860030a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Quality - Required Fields Check"
    }
   },
   "outputs": [],
   "source": [
    "def validate_required_fields(\n",
    "    df: 'pyspark.sql.DataFrame', \n",
    "    required_fields: list[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Validates the presence and non-nullity of required fields in a Spark DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The DataFrame to validate.\n",
    "    required_fields : list of str\n",
    "        List of required column names to check for existence and non-null values.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If any required columns are missing or contain null values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Prints the status of required columns and null value checks.\n",
    "    - Raises an error if validation fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    missing_columns = [field for field in required_fields if field not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Important columns are missing: {missing_columns}\")\n",
    "    print(\"All required columns are present\")\n",
    "\n",
    "   \n",
    "    print(\"\\nCheck null values on fields:\")\n",
    "    null_counts = df.select([\n",
    "        count(when(col(c).isNull(), c)).alias(c) for c in required_fields\n",
    "    ]).collect()[0]\n",
    "\n",
    "    has_nulls = False\n",
    "    for field in required_fields:\n",
    "        null_count = null_counts[field]\n",
    "        if null_count > 0:\n",
    "            print(f\" {field}: {null_count} null values\")\n",
    "            has_nulls = True\n",
    "        else:\n",
    "            print(f\"{field}: haven't null values\")\n",
    "\n",
    "    if has_nulls:\n",
    "        raise ValueError(\"There are null values in required fields\")\n",
    "\n",
    "    print(\"\\nQuality Success! All required columns are present and haven't null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b647e83d-b6e4-4bb3-ba9c-4163f2df2709",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fuction to Write Table"
    }
   },
   "outputs": [],
   "source": [
    "def write_table(\n",
    "    df: 'pyspark.sql.DataFrame', \n",
    "    catalog: str, \n",
    "    schema: str, \n",
    "    table: str, \n",
    "    mode: str = \"append\",\n",
    "    catalog_location: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write a Spark DataFrame to a Delta table in Databricks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The DataFrame to be written.\n",
    "    catalog : str\n",
    "        The catalog name (e.g., 'hive_metastore').\n",
    "    schema : str\n",
    "        The schema/database name.\n",
    "    table : str\n",
    "        The table name.\n",
    "    mode : str, optional\n",
    "        Write mode: 'append' (default) or 'overwrite'.\n",
    "    catalog_location : str, optional\n",
    "        Optional location for the catalog (not used in this function).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the mode is not 'append' or 'overwrite'.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - In 'append' mode, adds an 'execution_date' column (current date) and partitions by it.\n",
    "    - Uses Delta format and overwrites schema if needed.\n",
    "    \"\"\"\n",
    "    table_full = f\"{catalog}.{schema}.{table}\"\n",
    "    if mode == \"append\":\n",
    "        df = df.withColumn(\"execution_date\", current_timestamp())\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"execution_date\") \\\n",
    "            .option(\"overwriteSchema\", \"false\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_full)\n",
    "    elif mode == \"overwrite\":\n",
    "        df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(table_full)\n",
    "    else:\n",
    "        raise ValueError(\"Mode of write table doesn't support. You must use 'append' or 'overwrite'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7f08e1-4919-4e5c-8446-f8588e93ba92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function Main"
    }
   },
   "outputs": [],
   "source": [
    "def process_api() -> None:\n",
    "    \"\"\"\n",
    "    Fetches brewery data from the Open Brewery DB API, validates required fields, and writes the data to Delta Lake.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Fetches brewery data using the fetch_breweries_from_api function.\n",
    "        2. Creates a Spark DataFrame with the expected schema.\n",
    "        3. Validates the presence and non-nullity of required fields.\n",
    "        4. Writes the validated DataFrame to a Delta Lake table in append mode, partitioned by execution date.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If required columns are missing or contain null values.\n",
    "    Exception\n",
    "        For any other errors during the process, prints the error message.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    "    )\n",
    "    required_fields = [\n",
    "        'id', 'name', 'brewery_type', 'city', \n",
    "        'state_province', 'postal_code', 'country'\n",
    "    ]\n",
    "    try:\n",
    "        logging.info(\"Step 1: Fetching brewery data from API...\")\n",
    "        data = fetch_breweries_from_api()\n",
    "        logging.info(f\"Step 1 complete: Fetched {len(data)} records.\")\n",
    "\n",
    "        logging.info(\"Step 2: Creating Spark DataFrame with expected schema...\")\n",
    "        df_breweries = create_brewery_df(data)\n",
    "        logging.info(\"Step 2 complete: DataFrame created.\")\n",
    "        df_breweries.printSchema()\n",
    "\n",
    "        if df_breweries.rdd.isEmpty():\n",
    "            logging.error(\"Step 2 error: DataFrame is empty. No data to process or write.\")\n",
    "            raise Exception(\"DataFrame is empty. No data to process or write.\")\n",
    "\n",
    "        logging.info(\"Step 3: Validating required fields for presence and non-nullity...\")\n",
    "        validate_required_fields(df_breweries, required_fields)\n",
    "        logging.info(\"Step 3 complete: Required fields validated.\")\n",
    "\n",
    "        logging.info(\"Step 4: Writing DataFrame to Delta Lake table...\")\n",
    "        write_table(\n",
    "            df_breweries, \n",
    "            \"ab_inbev_lake\", \n",
    "            \"bronze\", \n",
    "            \"breweries_api_data\", \n",
    "            mode=\"append\"\n",
    "        )\n",
    "        logging.info(\"Step 4 complete: Data written to Delta Lake table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in process_api: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c466e0-6539-4e05-a9d9-6ab04842795c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call Fuction"
    }
   },
   "outputs": [],
   "source": [
    "process_api()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6565868492765546,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fetch_api_breweries",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
